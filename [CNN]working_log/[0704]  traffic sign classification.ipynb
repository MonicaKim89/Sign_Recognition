
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de0a7f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1307267417251797436\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4138026608950009926\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9883535296\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 1637534499432807704\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1710325966508194891\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from IPython.display import display\n",
    "import PIL\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eeb4bc",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0ceb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"myData\" # folder with all the class folders\n",
    "labelFile = 'labels.csv' # file with all names of classes\n",
    "batch_size_val=50  # how many to process together\n",
    "steps_per_epoch_val=2000\n",
    "epochs_val=10\n",
    "imageDimesions = (32,32,3)\n",
    "testRatio = 0.2    # if 1000 images split will 200 for testing\n",
    "validationRatio = 0.2 # if 1000 images 20% of remaining 800 will be 160 for validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9546ce",
   "metadata": {},
   "source": [
    "### Importing of the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95832438",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "images = []\n",
    "classNo = []\n",
    "myList = os.listdir(path)\n",
    "print(\"Total Classes Detected:\",len(myList))\n",
    "noOfClasses=len(myList)\n",
    "print(\"Importing Classes.....\")\n",
    "for x in range (0,len(myList)):\n",
    "    myPicList = os.listdir(path+\"/\"+str(count))\n",
    "    for y in myPicList:\n",
    "        curImg = cv2.imread(path+\"/\"+str(count)+\"/\"+y)\n",
    "        images.append(curImg)\n",
    "        classNo.append(count)\n",
    "    print(count, end =\" \")\n",
    "    count +=1\n",
    "print(\" \")\n",
    "images = np.array(images)\n",
    "classNo = np.array(classNo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21822512",
   "metadata": {},
   "source": [
    "### Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80b1413",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio)\n",
    " \n",
    "# X_train = ARRAY OF IMAGES TO TRAIN\n",
    "# y_train = CORRESPONDING CLASS ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0951a980",
   "metadata": {},
   "source": [
    "### TO CHECK IF NUMBER OF IMAGES MATCHES TO NUMBER OF LABELS FOR EACH DATA SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a89603",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Shapes\")\n",
    "print(\"Train\",end = \"\");print(X_train.shape,y_train.shape)\n",
    "print(\"Validation\",end = \"\");print(X_validation.shape,y_validation.shape)\n",
    "print(\"Test\",end = \"\");print(X_test.shape,y_test.shape)\n",
    "assert(X_train.shape[0]==y_train.shape[0]), \"The number of images in not equal to the number of lables in training set\"\n",
    "assert(X_validation.shape[0]==y_validation.shape[0]), \"The number of images in not equal to the number of lables in validation set\"\n",
    "assert(X_test.shape[0]==y_test.shape[0]), \"The number of images in not equal to the number of lables in test set\"\n",
    "assert(X_train.shape[1:]==(imageDimesions)),\" The dimesions of the Training images are wrong \"\n",
    "assert(X_validation.shape[1:]==(imageDimesions)),\" The dimesionas of the Validation images are wrong \"\n",
    "assert(X_test.shape[1:]==(imageDimesions)),\" The dimesionas of the Test images are wrong\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08aace",
   "metadata": {},
   "source": [
    "### READ CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4793c22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(labelFile)\n",
    "print(\"data shape \",data.shape,type(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2981f963",
   "metadata": {},
   "source": [
    "### DISPLAY SOME SAMPLES IMAGES  OF ALL THE CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ed30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_samples = []\n",
    "cols = 5\n",
    "num_classes = noOfClasses\n",
    "fig, axs = plt.subplots(nrows=num_classes, ncols=cols, figsize=(5, 300))\n",
    "fig.tight_layout()\n",
    "for i in range(cols):\n",
    "    for j,row in data.iterrows():\n",
    "        x_selected = X_train[y_train == j]\n",
    "        axs[j][i].imshow(x_selected[random.randint(0, len(x_selected)- 1), :, :], cmap=plt.get_cmap(\"gray\"))\n",
    "        axs[j][i].axis(\"off\")\n",
    "        if i == 2:\n",
    "            axs[j][i].set_title(str(j)+ \"-\"+row[\"Name\"])\n",
    "            num_of_samples.append(len(x_selected))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9879a56",
   "metadata": {},
   "source": [
    "### DISPLAY A BAR CHART SHOWING NO OF SAMPLES FOR EACH CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32fce88",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_of_samples)\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.bar(range(0, num_classes), num_of_samples)\n",
    "plt.title(\"Distribution of the training dataset\")\n",
    "plt.xlabel(\"Class number\")\n",
    "plt.ylabel(\"Number of images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369f03ca",
   "metadata": {},
   "source": [
    "### PREPROCESSING THE IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50085877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grayscale(img):\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    return img\n",
    "def equalize(img):\n",
    "    img =cv2.equalizeHist(img)\n",
    "    return img\n",
    "def preprocessing(img):\n",
    "    img = grayscale(img)     # CONVERT TO GRAYSCALE\n",
    "    img = equalize(img)      # STANDARDIZE THE LIGHTING IN AN IMAGE\n",
    "    img = img/255            # TO NORMALIZE VALUES BETWEEN 0 AND 1 INSTEAD OF 0 TO 255\n",
    "    return img\n",
    " \n",
    "X_train=np.array(list(map(preprocessing,X_train)))  # TO IRETATE AND PREPROCESS ALL IMAGES\n",
    "X_validation=np.array(list(map(preprocessing,X_validation)))\n",
    "X_test=np.array(list(map(preprocessing,X_test)))\n",
    "cv2.imshow(\"GrayScale Images\",X_train[random.randint(0,len(X_train)-1)]) # TO CHECK IF THE TRAINING IS DONE PROPERLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68189956",
   "metadata": {},
   "source": [
    "### ADD A DEPTH OF 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d367d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
    "X_validation=X_validation.reshape(X_validation.shape[0],X_validation.shape[1],X_validation.shape[2],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b730e92",
   "metadata": {},
   "source": [
    "### AUGMENTATAION OF IMAGES: TO MAKEIT MORE GENERIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269618bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen= ImageDataGenerator(width_shift_range=0.1,   # 0.1 = 10%     IF MORE THAN 1 E.G 10 THEN IT REFFERS TO NO. OF  PIXELS EG 10 PIXELS\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.2,  # 0.2 MEANS CAN GO FROM 0.8 TO 1.2\n",
    "                            shear_range=0.1,  # MAGNITUDE OF SHEAR ANGLE\n",
    "                            rotation_range=10)  # DEGREES\n",
    "dataGen.fit(X_train)\n",
    "batches= dataGen.flow(X_train,y_train,batch_size=20)  # REQUESTING DATA GENRATOR TO GENERATE IMAGES  BATCH SIZE = NO. OF IMAGES CREAED EACH TIME ITS CALLED\n",
    "X_batch,y_batch = next(batches)\n",
    " \n",
    "# TO SHOW AGMENTED IMAGE SAMPLES\n",
    "fig,axs=plt.subplots(1,15,figsize=(20,5))\n",
    "fig.tight_layout()\n",
    " \n",
    "for i in range(15):\n",
    "    axs[i].imshow(X_batch[i].reshape(imageDimesions[0],imageDimesions[1]))\n",
    "    axs[i].axis('off')\n",
    "plt.show()\n",
    " \n",
    " \n",
    "y_train = to_categorical(y_train,noOfClasses)\n",
    "y_validation = to_categorical(y_validation,noOfClasses)\n",
    "y_test = to_categorical(y_test,noOfClasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b1a12b",
   "metadata": {},
   "source": [
    "### CONVOLUTION NEURAL NETWORK MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43c23a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myModel():\n",
    "    no_Of_Filters=60\n",
    "    size_of_Filter=(5,5) # THIS IS THE KERNEL THAT MOVE AROUND THE IMAGE TO GET THE FEATURES.\n",
    "                         # THIS WOULD REMOVE 2 PIXELS FROM EACH BORDER WHEN USING 32 32 IMAGE\n",
    "    size_of_Filter2=(3,3)\n",
    "    size_of_pool=(2,2)  # SCALE DOWN ALL FEATURE MAP TO GERNALIZE MORE, TO REDUCE OVERFITTING\n",
    "    no_Of_Nodes = 500   # NO. OF NODES IN HIDDEN LAYERS\n",
    "    model= Sequential()\n",
    "    model.add((Conv2D(no_Of_Filters,size_of_Filter,input_shape=(imageDimesions[0],imageDimesions[1],1),activation='relu')))  # ADDING MORE CONVOLUTION LAYERS = LESS FEATURES BUT CAN CAUSE ACCURACY TO INCREASE\n",
    "    model.add((Conv2D(no_Of_Filters, size_of_Filter, activation='relu')))\n",
    "    model.add(MaxPooling2D(pool_size=size_of_pool)) # DOES NOT EFFECT THE DEPTH/NO OF FILTERS\n",
    "    \n",
    "    model.add((Conv2D(no_Of_Filters//2, size_of_Filter2,activation='relu')))\n",
    "    model.add((Conv2D(no_Of_Filters // 2, size_of_Filter2, activation='relu')))\n",
    "    model.add(MaxPooling2D(pool_size=size_of_pool))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(no_Of_Nodes,activation='relu'))\n",
    "    model.add(Dropout(0.5)) # INPUTS NODES TO DROP WITH EACH UPDATE 1 ALL 0 NONE\n",
    "    model.add(Dense(noOfClasses,activation='softmax')) # OUTPUT LAYER\n",
    "    # COMPILE MODEL\n",
    "    model.compile(Adam(lr=0.001),loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c5f6f",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f90e1af6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-9d30d50a6090>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmyModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataGen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_validation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'myModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = myModel()\n",
    "print(model.summary())\n",
    "history=model.fit_generator(dataGen.flow(X_train,y_train,batch_size=batch_size_val),steps_per_epoch=steps_per_epoch_val,epochs=epochs_val,validation_data=(X_validation,y_validation),shuffle=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d581ba3",
   "metadata": {},
   "source": [
    "### PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e9eb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['training','validation'])\n",
    "plt.title('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.figure(2)\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training','validation'])\n",
    "plt.title('Acurracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()\n",
    "score =model.evaluate(X_test,y_test,verbose=0)\n",
    "print('Test Score:',score[0])\n",
    "print('Test Accuracy:',score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3c614",
   "metadata": {},
   "source": [
    "### STORE THE MODEL AS A PICKLE OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8ff2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out= open(\"model_trained.p\",\"wb\")  # wb = WRITE BYTE\n",
    "pickle.dump(model,pickle_out)\n",
    "pickle_out.close()\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
